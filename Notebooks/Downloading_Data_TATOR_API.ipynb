{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "\n",
    "For downloading frames and their corresponding annotations. You'll need to provide your TATOR API key, which you can get \n",
    "from the TATOR website (see docs), to be able to download data associated with this project (70). Set the ROOT directory \n",
    "so all data will be saved in a specific location:\n",
    "\n",
    "```python\n",
    "ROOT\n",
    "   |\n",
    "   JPEGImages\n",
    "         |\n",
    "         Image_1.jpg\n",
    "         Image_2.jpg\n",
    "   |\n",
    "   Annotations\n",
    "         |\n",
    "         Label_2.xml\n",
    "         Label_2.xml\n",
    "```\n",
    "\n",
    "Ensure that you're token is not in the notebook when making a commit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing\n",
    "\n",
    "If you don't have the packages, run this cell and then restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tator tqdm pandas numpy pascal-voc-writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Install these using pip or conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pascal_voc_writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(frame, media_name, media_dir, api, media_id, download=False):\n",
    "    \"\"\"\n",
    "    Function is used to download specific frames into\n",
    "    the corresponding media folder.\n",
    "    \"\"\"\n",
    "    # Location of file on local machine\n",
    "    dst = media_dir + media_name.split(\".\")[0] + \"_\" + str(frame) + \".jpg\"\n",
    "    \n",
    "    # If it doesn't already exist, download, move.\n",
    "    if not os.path.exists(dst) and download:\n",
    "        src = api.get_frame(media_id, frames=[frame])\n",
    "        shutil.move(src, dst)\n",
    "        \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up\n",
    "\n",
    "Here we're setting the root directory where data will be downloaded. By default, it's just a folder called Data, located in the root of the repository. Then we create sub-folders containing Images, and Annotations. The basename for each image and annotation will correspond to each other, and just differ based on the file extension.\n",
    "\n",
    "Then we get the TATOR API token, which should be stored as an environmental variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root data location \n",
    "ROOT = \"./Data/\"\n",
    "IMAGE_DIR = ROOT + \"JPEGImages/\"\n",
    "LABEL_DIR = ROOT + \"Annotations/\"\n",
    "\n",
    "# This will create folders\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(LABEL_DIR, exist_ok=True)\n",
    "\n",
    "# Grabbing token from environmental variable\n",
    "MY_TOKEN = os.getenv(\"TATOR_TOKEN\")\n",
    "\n",
    "# Setting the api given the token, sanity check\n",
    "api = tator.get_api(host='https://cloud.tator.io', token=MY_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download?\n",
    "\n",
    "If you don't want to download the images, set this variable to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag for downloading images\n",
    "DOWNLOAD = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project ID\n",
    "\n",
    "The project ID can be found in the URL of the project on TATOR. You can only access specific projects given your API token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Media of Interest\n",
    "\n",
    "Here we're specifying the media we're interested in. Currently you have to add the name of the media by hand to create a list. However, you can also do some scripting to get all the media using `get_media_list`, and subset it somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the media we're intrested in\n",
    "MEDIA_LIST = [\n",
    "                \"Some_Media_Name.mov\"\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing all media from the project\n",
    "medias = api.get_media_list(PROJECT_ID)\n",
    "medias_names = [m.name for m in medias] \n",
    "\n",
    "print(f\"Number of medias: {len(medias)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for media in MEDIA_LIST:\n",
    "    if media in medias_names:\n",
    "        print(f\"This media WAS found: {media}\")\n",
    "    else:\n",
    "        print(f\"This media WASN'T found: {media}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Localizations for the Media of Interest\n",
    "\n",
    "Previously, we specified the media we're interested in, in `MEDIA_LIST`. Now we're getting all of the localizations that are associated with each of those medias. Note that there is difference in how TATOR stores localization annotations made using the Track Tool, and using the Create Localization Tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all class categories for all media\n",
    "ALL_CLASSES_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all media localization information\n",
    "media_w_localizations = []\n",
    "\n",
    "# Looping through each media, collecting information\n",
    "# from the media that we're intrested in\n",
    "for media in tqdm(medias):\n",
    "    \n",
    "    if media.name not in MEDIA_LIST:\n",
    "        continue\n",
    "        \n",
    "    print(\"Collecting metadata for: \", media.name)\n",
    "    \n",
    "    # All localizations and tracks for this media \n",
    "    localizations = api.get_localization_list(PROJECT_ID, media_id=[media.id])\n",
    "    tracks = api.get_state_list(PROJECT_ID, media_id=[media.id])\n",
    "    \n",
    "    # Tracks can contain localization information, but not always\n",
    "    # as some tracks are for other things. Here we look for \n",
    "    # the class categories stored in eiter track localizations\n",
    "    # (if they exist), and in the actual localizations\n",
    "    \n",
    "    # A dict containing the class category for this\n",
    "    # media's localization, based on it's ID\n",
    "    loc_id_to_class = {}\n",
    "    \n",
    "    # A list to hold all localizations for this media,\n",
    "    # whether they are from tracks or otherwise\n",
    "    media_localizations = []\n",
    "    \n",
    "    # Loop trhough the tracks\n",
    "    for track in tracks:\n",
    "        # Loop through all track localizations\n",
    "        for localization in track.localizations:\n",
    "            if \"ScientifcName\" in localization.attributes:\n",
    "                class_category = localization.attributes['ScientificName']\n",
    "                loc_id_to_class[localization.id] = class_category\n",
    "                media_localizations.append(localization)\n",
    "                ALL_CLASSES_LIST.append(class_category)\n",
    "    \n",
    "    # Now we look at all the actual localizations that are not tracks\n",
    "    for localization in localizations:\n",
    "        if \"ScientificName\" in localization.attributes:\n",
    "            class_category = localization.attributes['ScientificName']\n",
    "            loc_id_to_class[localization.id] = class_category\n",
    "            media_localizations.append(localization)\n",
    "            ALL_CLASSES_LIST.append(class_category)\n",
    "            \n",
    "    # Storing the media metadata we'll need later and the localizations\n",
    "    media_w_localizations.append([media.name.replace(\":\", \"_\"),\n",
    "                                  media.id,\n",
    "                                  media.width,\n",
    "                                  media.height,\n",
    "                                  media_localizations,\n",
    "                                  loc_id_to_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of class categories in these media\n",
    "pd.DataFrame(ALL_CLASSES_LIST).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Localizations\n",
    "\n",
    "If you notice, we store the localizations in the 4th index. So in this cell, we're accessing the first media in `MEDIA_LIST`, and the first localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_w_localizations[0][4][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Images and Annotations\n",
    "\n",
    "In this cell, we're using the list of media with localizations to download JUST the frames with localizations, and of course, the localizations themselves. If you set `DOWNLOAD` to False, then the images won't actually be downloaded, but the annotations will be. This may be useful, as you may not actually need to images themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np array \n",
    "media_w_localizations = np.array(media_w_localizations, dtype=object)\n",
    "\n",
    "# Here we're going through the media with localizations, creating\n",
    "# a media folder within ROOT, and then downloading just the frames\n",
    "# that have localizations (ignoring those that don't, for now)\n",
    "\n",
    "for index in range(len(media_w_localizations)):\n",
    "    \n",
    "    # Variables for easy access\n",
    "    media = media_w_localizations[index]\n",
    "    \n",
    "    media_name = media[0]\n",
    "    media_id = media[1]\n",
    "    media_width = media[2]\n",
    "    media_height = media[3]\n",
    "    localizations = media[4]\n",
    "    loc_id_to_class = media[5]\n",
    "    \n",
    "    print(\"Downloading frames and annotations for: \", media_name)\n",
    "    \n",
    "    # Download all the frames that have localizations. Images will be downloaded\n",
    "    # on multiple threads to save time.\n",
    "    frames_w_localizations = list(set([l.frame for l in localizations]))\n",
    "    \n",
    "    # Downloading each image frame with localizations, using multithreading\n",
    "    # If DOWNLOAD is False, then this will just pass back the path to where the \n",
    "    # image would have been saved, which is later stored in the XML file.\n",
    "    with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        img_paths = [executor.submit(download_image, frame, media_name, IMAGE_DIR, api, media_id, DOWNLOAD) for \n",
    "                     frame in frames_w_localizations] \n",
    "    \n",
    "    # Contains a list of frame paths on local machine\n",
    "    img_paths = [future.result() for future in img_paths]\n",
    "        \n",
    "    # This will hold a modified version of the localizations\n",
    "    # which will later be converted to a json and saved.\n",
    "    locs = []\n",
    "    \n",
    "    # Loop through the localizations for this media\n",
    "    # Creating a dict that will be used to create the XML file\n",
    "    for localization in localizations:\n",
    "        \n",
    "        # Variables for easy access\n",
    "        frame_id = localization.frame\n",
    "        frame_path = IMAGE_DIR + media_name.split(\".\")[0] + \"_\" + str(frame_id) + \".jpg\"\n",
    "        \n",
    "        # Converting the bounding boxes locations from\n",
    "        # normalized to relative, and in xyxy format\n",
    "        xmin = int(localization.x * media_width)\n",
    "        ymin = int(localization.y * media_height)\n",
    "        xmax = int(localization.width * media_width) + xmin\n",
    "        ymax = int(localization.height * media_height) + ymin\n",
    "\n",
    "        bbox = {\n",
    "            'xmin': xmin,\n",
    "            'ymin': ymin,\n",
    "            'xmax': xmax,\n",
    "            'ymax': ymax,\n",
    "        }\n",
    "        \n",
    "        # Converting to a dict, adding variables\n",
    "        loc = localization.to_dict()\n",
    "        loc['bbox'] = bbox\n",
    "        loc['path'] = frame_path\n",
    "        \n",
    "        # Not sure what's happening here. In TATOR, these localizations\n",
    "        # exist and have scientific names, but it's pulling up empty\n",
    "        # for a very small amount. Check this again in future...\n",
    "        try:\n",
    "            loc['class_category'] = loc_id_to_class[localization.id] \n",
    "        except:\n",
    "            loc['class_category'] = \"Unknown\" \n",
    "\n",
    "        locs.append(loc)\n",
    "        \n",
    "    # Here we're creating a dict that contains all frames\n",
    "    # as the key, and for each key (the frame), the localizations\n",
    "    # will be stored within a nested dict. For example:\n",
    "    # dict['frame_1'] = {localization_1, localization_2}\n",
    "    # dict['frame_2'] = {localization_1}\n",
    "    # dict['frame_3'] = {localization_1, localization_2...}\n",
    "    frame_localizations = {}\n",
    "\n",
    "    for entry in locs:\n",
    "        frame = entry['frame']\n",
    "        if frame in frame_localizations:\n",
    "            frame_localizations[frame].append(entry)\n",
    "        else:\n",
    "            frame_localizations[frame] = [entry]\n",
    "    \n",
    "    # Exporting this nested dict as VOC pascal .xml file with the same\n",
    "    # filename as the corresponding image.\n",
    "    for frame in tqdm(frames_w_localizations):\n",
    "        \n",
    "        # Grabbing localizations for just this frame\n",
    "        locs = frame_localizations[frame]\n",
    "        \n",
    "        # Location of file on local machine\n",
    "        img_path = locs[0]['path']\n",
    "        xml_path = LABEL_DIR + os.path.basename(img_path).replace(\".jpg\", \".xml\")\n",
    "        \n",
    "        # Create a writer object for just this frame\n",
    "        writer = Writer(img_path, width=media_width, height=media_height)\n",
    "\n",
    "        # loop through all localizations for this frame, add with writer\n",
    "        for loc in locs:\n",
    "        \n",
    "            bbox = loc['bbox']\n",
    "\n",
    "            writer.addObject(loc['class_category'],\n",
    "                             xmin=bbox['xmin'],\n",
    "                             ymin=bbox['ymin'],\n",
    "                             xmax=bbox['xmax'],\n",
    "                             ymax=bbox['ymax'])\n",
    "            \n",
    "        writer.save(xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1de9c0aaf7e968c1aef41e20f1ad1b01d1167fa61651abc97e5c5ef477aac459"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
